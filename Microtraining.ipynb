{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Microtraining - Data-Analysis with Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Getting started with feed forward neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Checking your Jupyter notebook\n",
    "# Press Ctrl+Enter to run me\n",
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This Training will show you the basics steps you will need to perform, to get started with Deep Learning on your dataset. The main goal is to give a short introduction into the most important stages of data analysis with supervised neural networks. \n",
    "\n",
    "The main stages are: \n",
    "* Data preprocessing\n",
    "* Model setup\n",
    "* Model training\n",
    "* Model testing\n",
    "* (Inference)\n",
    "* (Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Tensorflow](https://upload.wikimedia.org/wikipedia/commons/1/11/TensorFlowLogo.svg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<sup>Imagesource:https://upload.wikimedia.org/wikipedia/commons/1/11/TensorFlowLogo.svg</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Tensorflow](https://www.tensorflow.org/) is a opensource framework provided and developed for machine and deep learning. The main purpose is to provide an easy and effective way to manipulate and calculate tensors.  \n",
    "\n",
    "One of the key features of tensorflow is the ability to build computation graphs. These computation graphs can be build by using a High-Level language like Python. Afterwards these graphs are than initiated and executed in C++.\n",
    "The image below shows such a typical computation. A Input Tensor \"flows\" through the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://www.tensorflow.org/images/tensors_flowing.gif\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<sup>Imagesource:https://www.tensorflow.org/images/tensors_flowing.gif</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" width=\"300\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<sup>Imagesource:https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Keras](https://keras.io/) is a wrapper library, which provides an easy and highly abstracted interface for low level frameworks like Tensorflow. Standard layers like Dense layers or CNN layers are already included in Keras making it a very good choice if you want to build fast prototype networks. Of course the great flexibilty of Tensorflow gets lost, when you use Keras but it is possible to combine these two frameworks since Keras is basically just a wrapper for Tensorflow.   \n",
    "\n",
    "\n",
    "In this workshop we will just import Tensorflow since Keras is part of tensorflow since Tensorflow 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LetÂ´s code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this micro training we are going to build a supervised classification alghorithmn to detect a focus shift during lasercutting.\n",
    "The following parts will be build:\n",
    "* Preprocessing Pipeline\n",
    "* Neural Network Model\n",
    "* Loss calculation\n",
    "* Optimizer\n",
    "* Training\n",
    "* Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#imports for the scripts\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data preprocessing pipeline\n",
    "\n",
    "* **Preprocessing Pipeline**\n",
    "* Neural Network Model\n",
    "* Loss calculation\n",
    "* Optimizer\n",
    "* Training\n",
    "* Evaluation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Data preprocessing is all about getting your data ready for your analysis. Normally this step takes about 80%-90% of your development time. \n",
    "\n",
    "When building neural networks you typically perform the following steps during this process:\n",
    "* Data loading\n",
    "* (Removing unwanted or corrupted data)\n",
    "* Creating Labels\n",
    "* Combine datasets\n",
    "* Shuffel\n",
    "* Data normalization\n",
    "* Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The dataset\n",
    "<img src=\"./pics_jupyter/Experimentaufbau.png\">\n",
    "\n",
    "* 4000 PNG Images\n",
    "* 2 Classes (-1mm and 2mm)\n",
    "* seperated by folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The given Dataset was produced on a Tumpf TruLas 5030 Lasercutting System. A highspeed Camerasystem was used to monitor the meltpool behaviour during the cut. In this dataset 2 different cuts were made. The focus position of the Laserbeam was varied (-1mm and 2mm).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data loading\n",
    "\n",
    "Our dataset is loaded by a prewritten loader. \n",
    "\n",
    "The loader is iterating over every element of the folders [fokus+2](./fokus+2) and [fokus-1](./fokus-1) and loading it with OpenCV. \n",
    "\n",
    "Afterwards it returns two numpy arrays. The returned arrays have the dimensions `(Framenumber,Width,Height)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# import dataset loader\n",
    "from PictureLoader import PictureLoader\n",
    "# import clear for console\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# load the dataset and clear Output afterwards\n",
    "Loader = PictureLoader()\n",
    "pics_1, pics_2 = Loader.LoadPictures()\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Change me for different frames\n",
    "pic1_frame = 0\n",
    "pic2_frame = 0\n",
    "\n",
    "# Show dimensions of the Datasets\n",
    "print(\"Focus 1 Dimensions \", pics_1.shape)\n",
    "print(\"Focus 2 Dimensions \", pics_2.shape)\n",
    "\n",
    "#Plotting\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(\"Focus -1\")\n",
    "plt.imshow(pics_1[pic1_frame,:,:],cmap=\"hot\")\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(\"Focus 2\")\n",
    "_ = plt.imshow(pics_2[pic2_frame,:,:],cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create Labels\n",
    "\n",
    "To build supervised learning algorithms we need labeled data. Since we are performing a classification we will use ***one-hot-encoding***. \n",
    "<img src=\"./pics_jupyter/OneHotEncoding.PNG\" width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Gernerate Labels for the dataset in onehot encoding\n",
    "labels_1 = np.repeat([[1,0]],pics_1.shape[0],axis=0)\n",
    "labels_2 = np.repeat([[0,1]],pics_2.shape[0],axis=0)\n",
    "print(\"Labels 1 first example\\n\",labels_1[0],\"\\n\")\n",
    "print(\"Labels 2 first example\\n\",labels_2[0],\"\\n\")\n",
    "print(\"Labels 1 Dimension\\n\",labels_1.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combine Datasets\n",
    "\n",
    "To feed labeled and loaded datasets into our network we need to merge these datasets. This is done by numpy's `append` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create combined dataset\n",
    "images = np.append(pics_1,pics_2,axis=0)\n",
    "labels = np.append(labels_1,labels_2,axis=0)\n",
    "print(\"Image dimensions after merging\",images.shape)\n",
    "print(\"Label dimensions after merging\",labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shuffle dataset\n",
    "\n",
    "Normally networks are trained in batches. In cases where one label of the dataset is bigger than our batch size it can happen that the optimization algorithm converges to a local minima only found in one of the labeled datasets. To prevend this kind of behaviour the datasets are shuffled to minimize the effect of label depending minimas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(images)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data normalization\n",
    "Most machine learning algorithmns will not work correctly when the datarange of your input data differs too much from your selected weights. Therefore we will normalise the datarange of the input data.\n",
    "\n",
    "The maximum value of a grey scale pixel is $255$. The minimum is $0$  \n",
    "Dividing your dataset by $255$ scales your pixel values in a range between $0$ and $1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Data normalization\n",
    "images_normal = images / 255\n",
    "print(\"Max Value Images before normalization\", np.max(images))\n",
    "print(\"Max Value Images after normalization\", np.max(images_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Split dataset\n",
    "In order to test and evaluate our network after creation we need to create a _\"benchmark\"_ dataset. We do this by splitting our data into 2 different datasets. A train- and a test-dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# How much Traindata do you want?\n",
    "train_percentage = 0.9\n",
    "\n",
    "# Split your data\n",
    "split = int(images_normal.shape[0]*train_percentage)\n",
    "train_images = images_normal[:split]\n",
    "test_images = images_normal[split:]\n",
    "train_labels = labels[:split]\n",
    "test_labels = labels[split:]\n",
    "# Logging\n",
    "print(\"Train-dataset:\")\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(\"\\nTest-dataset:\")\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Network model\n",
    "* Preprocessing Pipeline\n",
    "* **Neural Network Model**\n",
    "* Loss calculation\n",
    "* Optimizer\n",
    "* Training\n",
    "* Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layers \n",
    "The model defines the overall architecture of the neural network. In the model you can define different layers and there sizes.\n",
    "Common layers in a network are:\n",
    "* Downscaling layers\n",
    "$$ x^{m  \\times  n} \\rightarrow x^{(\\frac{m}{s} \\times \\frac{n}{s})} $$ \n",
    "\n",
    "\n",
    "* Reshape/Flatten layers\n",
    "$$ x^{m \\times n} \\leftrightarrow x^{(m + n)} $$ \n",
    "\n",
    "\n",
    "* Dense layers \n",
    "$$ y^n =  f(W^{m x n}* x^m +b^n) $$\n",
    "\n",
    "\n",
    "* (Dropout layers)\n",
    "\n",
    "\n",
    "* (CNN layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here you can find a list of all standard layers implemented in [Keras](https://keras.io/layers/core/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation functions\n",
    "The activation function $f$ in dense layers is used to determine the output of this layer. You can use a dense layer as a hidden or a output layer:\n",
    "\n",
    "Commonly used functions are:\n",
    "\n",
    "\n",
    "* Sigmoid\n",
    "$$ \\frac{e^x}{e^x+1} $$\n",
    "\n",
    "\n",
    "* ReLU\n",
    "$$ max(0,x) $$\n",
    "\n",
    "\n",
    "* Softmax (Classification Output layer)\n",
    "$$ y_i= \\frac{e^{g_i}}{\\sum_j{e^{g_j}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "List of all implemented [Activations](https://keras.io/activations/) in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Build Architecture\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "#Downsampling\n",
    "  tf.keras.layers.Reshape((256,256,1),input_shape=(256,256)),\n",
    "  tf.keras.layers.AveragePooling2D(pool_size=(4, 4)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "\n",
    "  tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
    "\n",
    "  tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function and Optimization\n",
    "* Preprocessing Pipeline\n",
    "* Neural Network Model\n",
    "* **Loss calculation**\n",
    "* **Optimizer**\n",
    "* Training\n",
    "* Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "The loss function of your neural network defines the relationship between your true label $\\bar{y}$ and your predicted label $y$. By defining a loss function $f$ and setting this loss to $0$ a optimization problem is created.\n",
    "\n",
    "$$f(y(w,x),\\bar{y}(x))\\stackrel{!}{=}0$$\n",
    "\n",
    "\n",
    "The most commonly used for categorisations functions are:\n",
    "\n",
    "* Mean squared error:\n",
    "$$ \\sum_{ i=0 }^n { \\frac{ (y_{ i }-\\bar{y_{i}})^2}{ n } } $$\n",
    "\n",
    "* Categorical cross entropy:\n",
    "$$ -\\sum_{i=0}^n {y_{i}*\\log (\\bar{y_{i}}) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Keras already has a lot loss functions implemented. Take a look [here](https://keras.io/losses/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization  \n",
    "\n",
    "The definition of your network determines size of your network directly. A single dense layer creates $ m \\cdot n + n $ free variables. Since we are using non linear functions like **ReLU** as activation functions we can not simply solve these equation systems.\n",
    "\n",
    "The Optimizer plays a crutial role during your training phase. It is responsible for changing your weights $W$ and your bias $b$ the \"right\" amount after every iteration. \n",
    "\n",
    "For gradient descent the most important parameters is the **learning rate**\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*rBQI7uBhBKE8KT-X.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<sup>Imagesource:https://cdn-images-1.medium.com/max/1600/0*rBQI7uBhBKE8KT-X.png</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You can find more [optimizers](https://keras.io/optimizers/) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model compilation\n",
    "\n",
    "loss_function = \"mean_squared_error\"\n",
    "optimizer = tf.keras.optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer,\n",
    "              loss=loss_function,\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training\n",
    "* Preprocessing Pipeline\n",
    "* Neural Network Model\n",
    "* Loss calculation\n",
    "* Optimizer\n",
    "* **Training**\n",
    "* Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "In this step we feed our data into out neural network and change the weights iterativly. The most important factors during this phase are:\n",
    "\n",
    "* Batch Size  \n",
    "    How many samples do you want to load for each Train step  \n",
    "    \n",
    "    \n",
    "* Epochs  \n",
    "    How often do you want to train your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Logging\n",
    "from time import time\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"graph/{}\".format(time()))\n",
    "\n",
    "# Training\n",
    "_ = model.fit(train_images, train_labels, batch_size=100, epochs=2, verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "* Preprocessing Pipeline\n",
    "* Neural Network Model\n",
    "* Loss calculation\n",
    "* Optimizer\n",
    "* Training\n",
    "* **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "loss,acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Average loss of your test data:\\n\",loss)\n",
    "print(\"Accuracy of your prediction:\\n\",acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "predict_frame = 1000\n",
    "#Get true and predicted categories\n",
    "true_categorie = labels[predict_frame]\n",
    "predicted_categorie = model.predict(images_normal[predict_frame].reshape([1,256,256]))\n",
    "\n",
    "print(\"\\nTrue Categorie:\\n\",true_categorie)\n",
    "print(\"\\nPredicted Categorie:\\n\",predicted_categorie)\n",
    "_ = plt.imshow(pics_1[predict_frame,:,:],cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Run the whole Pipeline on your laptop. \n",
    "\n",
    "Change the `predict_frame` variable in your prediction cell to try out your new ai model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2\n",
    "Change it up!  \n",
    "Make sure to re-run your Model definition and compilation after each change.\n",
    "After that you can train and evaluate your model again.\n",
    "\n",
    "\n",
    "* Change the size of your layers by changing the node count of  `tf.keras.layers.Dense(100, activation=tf.nn.relu)` in your model definition  \n",
    "\n",
    "\n",
    "\n",
    "* Add more layers to your network by copying `tf.keras.layers.Dense(100, activation=tf.nn.relu)` (Hint: Watch out for the comma)\n",
    "\n",
    "\n",
    "* Increase the learning rate to 1\n",
    "\n",
    "\n",
    "* Increase the size of your network to at least 500 nodes for each layer. \n",
    "\n",
    "\n",
    "* Run your training phase with 10 instead of 3 epochs.\n",
    "\n",
    "\n",
    "* Remove your nomalization by removing `/255` from the Data normalization cell. Watch your accuracy and loss during training!  \n",
    "(Re-run every cell after that to reset your pipeline) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Watch what you did\n",
    "\n",
    "Tensorboard is Tensorflow's logging and monitoring webinterfaces. In this interface you can take a look at the accuracy and losses you scored for your different models. You can also visualize the Graph definition. \n",
    "\n",
    "\n",
    "Open your Tensorboard by running the **start_tensorboard.bat** file in the Anaconda prompt. Copy the URL that is given by the Program and paste it in your browser.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./pics_jupyter/Tensorboard.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced Exercise/Homework\n",
    "\n",
    "* Create a Convolutional neural network by including  [Conv2D](https://keras.io/layers/convolutional/#conv2d) and \n",
    "[MaxPooling2D](https://keras.io/layers/pooling/#maxpooling2d) layers. \n",
    "\n",
    "* Rewrite `PictureLoader.py`to use Python [Generators](https://wiki.python.org/moin/Generators) instead of loading the whole dataset at once. By using generators you can use bigger datasets than your RAM.\n",
    "\n",
    "* Download a new dataset from [Kaggle](https://www.kaggle.com/datasets) and create a new model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tipps & Tricks\n",
    "\n",
    "1. Speed up your training with a GPU acceleration! Install the Anaconda package with `conda install -c anaconda tensorflow-gpu` instead of pip to avoid complications!\n",
    "2. [How to combine Keras with Tensorflow](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html)\n",
    "3. Log what you are doing by using [GIT](https://git-scm.com/)\n",
    "4. Clean data is key!\n",
    "5. Have fun!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Download this Repository\n",
    "https://github.com/Smokrow/Microtraining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
